---
title: "Clinical Prediction Pipeline"
author: "JAS"
date: " "
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Demonstration: Comparison between Random Forest and Logistic Regression for Clinical Risk Scores

This demonstration once again uses the built-in NHANES data. You will use two different algorithms (random forestand logistic regression) to generate a clinical risk score for diabetes. We will then compare the two models in both accuracy and calibration

The steps we will follow are:

1. Load and subset the data.

2. Partition data into a 70/30 training/testing split.

3. Construct two models in the training set using each of the algorithms to predict diabetes. For the random forest, twe will use 3 different values of mtry. 

4. Compare accuracy across the two models in the training set. 

5. Output predicted probabilities from each of the models applied within the testing set. 

6. Plot and compare calibration curves across the two algorithms. 

7. Calibrate the predicted probabilites from Random Forest using two common methods.

8. Plot and compare the new calibration curves across the two algorithms.

***


Remember from our prior use of these data, they are imbalanced. We will need to deal with this during our analysis.


```{r data_prep}
library(lattice)
library(NHANES)
library(dplyr)
library(caret)
library(randomForest)
library(here)



data ("NHANES")
table(NHANES$Diabetes)

keep.varn <- names(NHANES) %in% c("Age", "Race1", "Education", "Poverty", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100", "BPSysAve", "BPDiaAve", "TotChol")

NHANES.subset<-NHANES[keep.var]

str(NHANES.subset)

#Remove missings and then remove duplicates
NHANES.subset<-na.omit(NHANES.subset)
NHANES.subset<-unique(NHANES.subset)

#Check distributions
summary(NHANES.subset)

```

### Set up: Partition data into training/testing

```{r partition}

set.seed(123)

training.data<-createDataPartition(NHANES.subset$Diabetes, p=0.7, list=F)
train.data<-NHANES.subset[training.data, ]
test.data<-NHANES.subset[-training.data, ]

```

### Model 1: Random Forest with 3 values of mtry and 3 values of ntree

For speed, we are using down sampling and 5-fold cross-validation. 

```{r randomforest}
# Try mtry of all, half of all, sqrt of all, 
# Try ntree of 100, 300, 500

feat.count<-c((ncol(train.data)-1), (ncol(train.data)-1)/2, sqrt(ncol(train.data)-1))
grid.rf<-expand.grid(mtry=feat.count)

control.obj<-trainControl(method="cv", number=5, sampling="down")

tree.num<-seq(100,500, by=200)
results.trees<-list()
for (ntree in tree.num){
  set.seed(123)
    rf.nhanes<-train(
                      Diabetes~., 
                      data=train.data, 
                      method="rf", 
                      trControl=control.obj, 
                      metric="Accuracy", 
                      tuneGrid=grid.rf, 
                      importance=TRUE, 
                      ntree=ntree)
    index<-toString(ntree)
  results.trees[[index]]<-rf.nhanes$results
}

output.nhanes<-bind_rows(results.trees, .id = "ntrees")
best.tune<-output.nhanes[which.max(output.nhanes[,"Accuracy"]),]
best.tune$mtry
results.trees

mtry.grid<-expand.grid(.mtry=best.tune$mtry)

set.seed(123)
    rf.nhanes.bt<-train(
                          Diabetes~., 
                          data=train.data, 
                          method="rf", 
                          trControl=control.obj, 
                          metric="Accuracy", 
                          tuneGrid=mtry.grid, 
                          importance=TRUE, 
                          ntree=as.numeric(best.tune$ntrees))

confusionMatrix(rf.nhanes.bt)
varImp(rf.nhanes.bt)
varImpPlot(rf.nhanes.bt$finalModel)
```


### Model 2: Logistic Regression
```{r logistic}

set.seed(123)

control.obj <- trainControl(method="cv", number=10, sampling="up")

logit.nhanes<-train(
                    Diabetes~., 
                    data=train.data, 
                    method="glm", 
                    family="binomial",
                    preProcess=c("center", "scale"), 
                    trControl=control.obj)

logit.nhanes$results
confusionMatrix(logit.nhanes)
coef(logit.nhanes$finalModel)
```

### Output predicted probabilities from each of the three models applied within the testing set. 

```{r predprob}
#Predict in test-set and output probabilities
rf.probs<-predict(rf.nhanes, test.data, type="prob")

#Pull out predicted probabilities for Diabetes=Yes
rf.pp<-rf.probs[,2]

#Predict in test-set using response type
logit.probs<-predict(logit.nhanes, test.data, type="prob")
logit.pp<-logit.probs[,2]
```
### Plot and compare calibration curves across the  algorithms. 

```{r plot}
pred.prob<-data.frame(Class=test.data$Diabetes, logit=logit.pp, rf=rf.pp)

calplot<-(calibration(Class ~ logit+rf, data=pred.prob, class="Yes", cuts=10))

xyplot(calplot, auto.key=list(columns=2))
```

### Calibrate the probabilities from  RF

Partition testing data into 2 sets: set to train calibration and then set to evaluate results

Method 1: Platt's Scaling-train a logistic regression model on the outputs of your classifier


```{r calibrate}

set.seed(123)
cal.data.index<-test.data$Diabetes%>% createDataPartition(p=0.5, list=F)
cal.data<-test.data[cal.data.index, ]
final.test.data<-test.data[-cal.data.index, ]

#Calibration of RF

#Predict on test-set without scaling to obtain raw pred prob in test set
rf.probs.nocal<-predict(rf.nhanes, final.test.data, type="prob")
rf.pp.nocal<-rf.probs.nocal[,2]

#Apply model developed on training data to calibration dataset to obtain predictions
rf.probs.cal<-predict(rf.nhanes, cal.data, type="prob")
rf.pp.cal<-rf.probs.cal[,2]

#Add to dataset with actual values from calibration data
calibrf.data.frame<-data.frame(rf.pp.cal, cal.data$Diabetes)
colnames(calibrf.data.frame)<-c("x", "y")

#Use logistic regression to model predicted probabilities from calibration data to actual vales
calibrf.model<-glm(y ~ x, data=calibrf.data.frame, family = binomial)

#Apply calibration model above to raw predicted probabilities from test set
data.test.rf<-data.frame(rf.pp.nocal)
colnames(data.test.rf)<-c("x")
platt.data.rf<-predict(calibrf.model, data.test.rf, type="response")

platt.prob.rf<-data.frame(Class=final.test.data$Diabetes, rf.platt=platt.data.rf, rf=rf.pp.nocal)

calplot.rf<-(calibration(Class ~ rf.platt+rf, data=platt.prob.rf, class="Yes", cuts=10))
xyplot(calplot.rf, auto.key=list(columns=2))



```


 Using the Demonstration Code as a guide, create clinical risk scores for diabetes using one of the regularized regression algorithms: either ridge, elastic net or lasso.
    
```{r}
set.seed(123)

#Create grid to search lambda
lambda <- 10^seq(-3,3, length=1000)

control.obj<-trainControl(method="cv", number=5, sampling="up")

#Note replacing tuneLength with tuneGrid
lasso.model<-train(
         Diabetes ~., data=train.data,
         method="glmnet", 
         trControl=control.obj, 
         preProc=c("center", "scale"), 
         tuneGrid=expand.grid(alpha = 1, lambda=lambda)
)

lasso.model$bestTune
  
# Model coefficients
coef(lasso.model$finalModel, lasso.model$bestTune$lambda)

# Make predictions in test set

lasso.pred <- lasso.model %>% predict(test.data)

# Model prediction performance
confusionMatrix(lasso.model)

```
```{r}
#Predict in test-set using response type
lasso.probs<-predict(lasso.model, test.data, type="prob")
lasso.pp<-lasso.probs[,2]
```


```{r}
pred.prob<-data.frame(Class=test.data$Diabetes, logit=logit.pp, rf=rf.pp, lasso = lasso.pp)

calplot<-(calibration(Class ~ logit + rf + lasso, data=pred.prob, class="Yes", cuts=10))

xyplot(calplot, auto.key=list(columns=2))
```

```{r}

set.seed(123)
cal.data.index<-test.data$Diabetes%>% createDataPartition(p=0.5, list=F)
cal.data<-test.data[cal.data.index, ]
final.test.data<-test.data[-cal.data.index, ]

#Calibration of RF

#Predict on test-set without scaling to obtain raw pred prob in test set
lasso.probs.nocal<-predict(lasso.model, final.test.data, type="prob")
lasso.pp.nocal<-lasso.probs.nocal[,2]

#Apply model developed on training data to calibration dataset to obtain predictions
lasso.probs.cal<-predict(lasso.model, cal.data, type="prob")
lasso.pp.cal<-lasso.probs.cal[,2]

#Add to dataset with actual values from calibration data
calib_lasso.data.frame<-data.frame(lasso.pp.cal, cal.data$Diabetes)
colnames(calib_lasso.data.frame)<-c("x", "y")

#Use logistic regression to model predicted probabilities from calibration data to actual vales
calib_lasso.model<-glm(y ~ x, data=calib_lasso.data.frame, family = binomial)

#Apply calibration model above to raw predicted probabilities from test set
data.test.lasso <- data.frame(lasso.pp.nocal)
colnames(data.test.lasso)<-c("x")
platt.data.lasso <-predict(calib_lasso.model, data.test.lasso, type="response")

platt.prob.lasso <-data.frame(Class=final.test.data$Diabetes, lasso.platt = platt.data.lasso, lasso = lasso.pp.nocal)

calplot.lasso <- (calibration(Class ~ lasso.platt + lasso, data = platt.prob.lasso, class="Yes", cuts=10))
xyplot(calplot.lasso, auto.key=list(columns=2))

```

    
    Compare your results to what was obtained to random forest. 
    
    Which features were most important in contributing to the clinical risk score?
    
    How useful would these clinical risk scores be in practice? What could improve them?
    
    - 
    
    - Depends on purpose - more data that is able to imporve descrimination
        - omics?
        - gentics?
        - family history?
        
