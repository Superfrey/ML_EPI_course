---
title: "Demonstration of Regularization"
author: "JAS"
date: ''
output:
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Demonstration of Regularization Methods

This will be a demonstration of the three regularization methods discussed: ridge regression, Lasso (least absolute shrinkage and selection operator) and Elastic Net.

## Description of Data

The data we will be using are from the 2019 U.S. County Health Rankings. They provide data on a number of demographic, social, environmental and health characteristics on counties within the United States. *We will be using this dataset to try to identify the most important predictors of life expectancy on a county-level.* We have restricted the dataset to 67 features and an outcome of life expectancy in years. 

Original data upon which this exercise has been based can be found here: http://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation

Variable names are not originally informative. You can look up all full variable name meanings here: http://www.countyhealthrankings.org/sites/default/files/2019%20Analytic%20Documentation_1.pdf


### Load needed libraries
```{r}
library(tidyverse) 
library(caret)
library(glmnet)
library(vroom)
```

### Step 1: Read in data, partition, and put features into separate object 

When using CreateDataPartition, note that for numeric y, the sample is split into groups sections based on percentiles and sampling is done within these subgroups. This helps training and testing to be similar. Default number of quantiles is 5.

We are partitioning the data in a 70/30 split.

```{r data_prep}
set.seed(123)

library(here)

chr <- vroom(here::here("data-raw/chr.csv"))

#Strip off ID Variable
chr<-chr[,2:68]

#Add informative feature names
var.names<-c("pre_death", "poorhealth", "poorphyshealth_days", "poormenthealth_days", "low_bwt", "ad_smoking", "ad_obesity", "foodenv_index", "phys_inactivity", "exer_access", "excess_drink", "alc_drivdeaths", "sti", "teen_birth", "uninsured", "primcareproviders", "dentists", "menthealthproviders", "prevhosp", "mammo_screen", "flu_vacc", "hsgrad", "somecollege", "unemployed", "child_poverty", "income_ineq", "sing_parent", "social_assoc", "violent_crime", "injury_deaths", "pm_air", "water_viol", "housing_prob", "driving_alone", "long_commute", "life_exp", "age_adj_premortality", "freq_physdistress", "freq_mentdistress", "diabetes", "hiv", "food_insecure", "ltd_access_healthyfood", "mvcrash_deaths", "insuff_sleep", "uninsured_adults", "uninsured_child", "other_pcp", "medhhinc", "freelunch_child", "res_seg_bw", "res_seg_nw", "firearm_fatalities", "homeownership", "hous_cost_burden", "population", "bw18", "gte65", "nonhisp_afam", "AmerInd_AlasNative", "Asian", "OPacIslander", "Hisp", "nonhisp_white", "nonprof_english", "female", "rural")

colnames(chr)<-var.names

chr$pre_death<-NULL
chr$age_adj_premortality<-NULL

train.indices<-createDataPartition(y=chr$life_exp,p=0.7,list=FALSE)
train.data<-chr[train.indices, ]
test.data<-chr[-train.indices, ]


```

### Step 2: Implementing regularized regression 

I will demonstrate regularized regression using the caret package. Note, it is the glmnet package that we will call to within caret. This package allows us to run all three of the penalized models using the same format. The value of the alpha parameter dictates whether it is a ride regression, lasso or elastic net. A value of 0 is the ridge regression, the 1 is a lasso and any value in between 0 and 1 will provide an elastic net. 

*** 

### Example 1: Using the defaults within caret

By default, caret will vary both alpha and lambda to select the best values via cross-validation. Because the alpha is not set at 0 or 1, this can (and often does) result in an elastic net. But, you can set the alpha level at a fixed value in order to obtain ridge or lasso results.

tuneLength sets the number of combinations of different values of alpha and lambda to compare. For example, setting tunelength to 10 will result in 10 values of alpha and 10 values of lambda


```{r enet}
set.seed(123)

en.model<- train(
                  life_exp ~., 
                  data = train.data, 
                  method = "glmnet",
                  trControl = trainControl("cv", number = 10),
                  preProc=c("center", "scale"),
                  tuneLength=10
                   )
#Print the values of alpha and lambda that gave best prediction
en.model$bestTune

#Print all of the options examined
en.model$results

# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda)

# Make predictions in test set

en.pred <- en.model %>% predict(test.data)

# Model prediction performance
postResample(en.pred,test.data$life_exp)

data.frame(
            RMSE = RMSE(en.pred, test.data$life_exp),
            Rsquare = R2(en.pred, test.data$life_exp)
          )


```

### Example 2: Fixing alpha to create a Ridge Regression

You can fix the alpha (I have it set to 0 for a ridge below) to obtain either a ridge or lasso analysis. 

You can also input your own tuning values. Below, the code creates a sequence of lambda values that you set yourself to use for tuning.

CONSIDERATION: If the caret package will select the optimal alpha and lambda value, why might you still choose lasso or ridge over elastic net (or an automated process of choosing alpha as in caret)? 


```{r tuning}
#Create grid to search lambda
lambda<-10^seq(-3,3, length=1000)

set.seed(123)

#Note replacing tuneLength with tuneGrid
ridge.model<-train(
         life_exp ~., data=train.data,
         method="glmnet", 
         trControl=trainControl("cv", number=10), 
         preProc=c("center", "scale"), 
         tuneGrid=expand.grid(alpha=0, lambda=lambda)
)

ridge.model$bestTune
  
# Model coefficients
coef(ridge.model$finalModel, ridge.model$bestTune$lambda)

# Make predictions in test set

ridge.pred <- ridge.model %>% predict(test.data)

# Model prediction performance
postResample(ridge.pred,test.data$life_exp)
data.frame(
  RMSE = RMSE(ridge.pred, test.data$life_exp),
  Rsquare = R2(ridge.pred, test.data$life_exp)
)
```

Exercise: using the code above as a guide, tune  a lasso model. Compare evaluation metrics in the test set to what you obtained above from the elastic net and ridge.

```{r}



```


